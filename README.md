Модуль AI-ассистента (Neo OSI Backend)

## 1. Общее описание

Данный модуль является ядром интеллектуального ассистента в проекте **Neo OSI**. Его основная задача - автоматизация процесса создания юридических и бухгалтерских документов через интуитивно понятный диалоговый интерфейс. Модуль предоставляет API, которое позволяет фронтенд-приложению (PWA) вести многошаговый диалог с пользователем, собирать необходимые данные и генерировать на их основе готовые `.docx` файлы.

Это решение позволяет значительно сократить время на рутинные операции, минимизировать ошибки человеческого фактора и предоставить пользователям круглосуточный сервис по подготовке документов.

## 2. Ключевые возможности

- **Билингвальная поддержка (RU/KZ):** Система автоматически определяет язык пользователя и ведет весь диалог (ответы, вопросы, инструкции) на соответствующем языке.
- **Stateful (контекстный) диалог:** В отличие от простых чат-ботов, ассистент "помнит" контекст беседы. Он управляет состоянием пользователя, позволяя гибко переключаться между заполнением разных документов или выходить в режим общей консультации.
- **Динамическая генерация документов:** На основе выбранного шаблона AI генерирует уникальный список вопросов, а после получения данных формирует готовый `.docx` файл.
- **RAG (Retrieval-Augmented Generation):** Для ответов на общие вопросы используется векторная база знаний, созданная на основе нормативных документов. Это позволяет ассистенту давать консультации, основываясь на реальных данных, а не на общей информации из интернета.
- **Интегрированная бизнес-логика:** Реализована система тарифных планов ("Базовый", "Премиум") с ежемесячными лимитами на количество генерируемых документов.

## 3. Технологический стек

- **Backend:** NestJS, TypeScript
- **AI & LLM:** Google Gemini API (модели `gemini-1.5-pro` и `gemini-1.5-flash`), LangChain.js
- **Базы данных:** PostgreSQL (для хранения состояния пользователя и истории чата), In-memory Vector Store (для RAG)
- **Генерация документов:** `docxtemplater`
- **Prompt Engineering:** Разработана система многоуровневых промптов для решения задач определения намерений, генерации вопросов и извлечения структурированных данных.

## 4. Настройка и запуск

### 4.1. Предварительные требования
- Установлены Node.js (v18+), npm/yarn, Docker.
- Репозиторий проекта склонирован (`git clone -b dev ...`).
- Запущен контейнер с базой данных (`docker-compose up -d`).

### 4.2. Конфигурация
1. Создайте файл `.env` в корневой директории проекта.
2. Скопируйте в него содержимое из `example.env` или воспользуйтесь шаблоном ниже.
3. **Обязательно** заполните переменную `GEMINI_API_KEY` вашим ключом доступа к Google AI Studio.

```env
# Ключ для доступа к Google Gemini API
GEMINI_API_KEY=ВАШ_СЕКРЕТНЫЙ_КЛЮЧ_GEMINI

# Настройки подключения к базе данных (соответствуют docker-compose.yml)
DB_HOST=localhost
DB_PORT=5433
DB_USERNAME=admin
DB_PASSWORD=mysecretpassword
DB_DATABASE=neo_osi_db

# Секретный ключ для подписи JWT токенов
JWT_SECRET=THIS_IS_A_VERY_SECRET_AND_LONG_KEY_FOR_JWT

### 4.3. Запуск и кэширование базы знаний

**1. Установка зависимостей**

Перед первым запуском необходимо установить все зависимости проекта. В корневой директории выполните команду:
```bash
npm install
```

**2. Запуск приложения в режиме разработки**

Для запуска сервера используйте следующую команду:
```bash
npm run start:dev
```

**Очень важный этап:** При первом запуске этой командой автоматически выполнится скрипт `createCache()` (находится в `src/main.ts`). Этот скрипт выполняет следующие действия:
-   Сканирует папки `knowledge_base` и `knowledge_base/templates/pdf_previews` на наличие PDF-файлов.
-   Извлекает из каждого PDF-файла чистый текст.
-   Сохраняет этот текст в специальную папку `.pdf-cache`.

Этот процесс называется **кэшированием** или **индексацией**. Созданные текстовые файлы затем используются для построения векторной базы знаний (RAG), которая позволяет AI-ассистенту отвечать на вопросы, основываясь на содержании ваших документов.

После завершения кэширования и инициализации всех модулей, вы увидите в консоли сообщение о том, что приложение успешно запущено. Сервис будет доступен по адресу `http://localhost:3000`.

## 5. API эндпоинты и использование

Вся коммуникация с AI-модулем происходит через один основной эндпоинт.

### `POST /ai/chat`
**Защита:** JWT (требуется Bearer Token в заголовке `Authorization`)

**Тело запроса:**
```json
{
  "prompt": "Текст сообщения от пользователя"
}
```

#### Пример полного цикла генерации документа:

**Шаг 1: Инициация диалога и получение вопросов**

Отправляем запрос с просьбой сгенерировать документ.

**Запрос (пример с использованием `curl`):**
```bash
curl -X POST http://localhost:3000/ai/chat \
-H "Content-Type: application/json" \
-H "Authorization: Bearer ВАШ_JWT_ТОКЕН" \
-d '{
      "prompt": "Помоги мне сделать акт приема-передачи технической документации"
    }'
```

**Ответ:**
Система вернет JSON с полем `action: 'collect_data'`, списком вопросов для заполнения и инструкциями.

```json
{
    "aiResponse": {
        "action": "collect_data",
        "requestId": "a1b2c3d4e5f6...",
        "templateName": "forma-akta-priyema-peredachi-tekhnicheskoy-dokumentatsii....docx",
        "questions": "Для заполнения документа 'Форма акта...' потребуется следующая информация:\n1. Адрес...\n2. ФИО отправителя...",
        "instructions": "Пожалуйста, предоставьте данные для документа. Если хотите отменить, напишите \"Отмена\"."
    }
}
```

**Шаг 2: Предоставление данных и получение файла**

Отправляем все собранные данные одним сообщением в поле `prompt`.

**Запрос (пример с использованием `curl`):**
```bash
curl -X POST http://localhost:3000/ai/chat \
-H "Content-Type: application/json" \
-H "Authorization: Bearer ВАШ_JWT_ТОКЕН" \
--output document.docx \
-d '{
      "prompt": "Адрес: г. Астана, ул. Достык, 5; ФИО отправителя: Иванов Иван Иванович, Директор; Документы: 1. Паспорт на 5 листах; 2. Акт осмотра на 2 листах; ..."
    }'
```
*(**Примечание:** флаг `--output document.docx` в `curl` сразу сохранит ответ в файл)*

**Ответ:**
Если все данные предоставлены корректно, сервер вернет ответ с `Content-Type: application/vnd.openxmlformats-officedocument.wordprocessingml.document` и телом, содержащим `.docx` файл. Если данных не хватает, он вернет JSON с уточняющими вопросами, как в Шаге 1.

## 6. Архитектура и логика работы

-   **Определение намерения (`Intent Detection`):** Первичный промпт определяет, хочет ли пользователь сгенерировать документ или задать общий вопрос.
-   **Управление состоянием:** Состояние пользователя (`IDLE` или `WAITING_FOR_DATA`) хранится в БД. Это позволяет `AiController` направлять запросы по разным логическим веткам.
-   **Извлечение данных (`Data Extraction`):** Для анализа ответов пользователя используется жесткий промпт, который заставляет AI работать в режиме "робота по извлечению данных", что минимизирует ошибки и "креативность" модели.
-   **RAG-поток:** Если намерение определено как "общий вопрос", система находит релевантные куски текста из векторной базы и передает их в AI вместе с вопросом для генерации ответа, основанного на фактах.

## 7. Потенциальные улучшения

-   **Сохранение "черновиков":** Реализовать механизм сохранения частично заполненных данных, чтобы пользователь мог вернуться к генерации документа позже.
-   **Более умная обработка неоднозначности:** Улучшить логику для случаев, когда пользовательский запрос на генерацию может соответствовать нескольким шаблонам (например, "сделай акт").
-   **Оптимизация токенов:** Внедрить кэширование для часто задаваемых вопросов и более агрессивное сжатие истории чата для снижения стоимости API-запросов.

```
